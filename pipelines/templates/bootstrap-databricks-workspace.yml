#
# Template that bootstraps the Databricks workspace for the data pipeline and project using Azure CLI and Databricks API calls.
#

parameters:
  - name: serviceConnection
    displayName: 'Azure Resource Manager service connection'
    type: string

  - name: resourceGroupName
    displayName: 'Azure Databricks Resource Group Name'
    type: string

  - name: projectGroupName
    displayName: 'Project User Group Name'
    type: string

  - name: dataServicePrincipalClientId
    displayName: 'Data Pipeline Service Principal Client ID'
    type: string

  - name: storageAccountName
    displayName: 'ADLS Storage Account Name'
    type: string

  - name: pipelineContainerName
    displayName: 'ADLS Filesystem Container for the Pipeline Data'
    type: string

  - name: projectContainerName
    displayName: 'ADLS Filesystem Container for the Project Data'
    type: string

  - name: dataFactoryName
    displayName: 'Azure Data Factory Name'
    type: string

  - name: databricksWorkspaceName
    displayName: 'Azure Databricks Workspace Name'
    type: string

  - name: databricksJobsPoolName
    displayName: 'Name of the Azure Databricks Jobs Instance Pool'
    type: string

  - name: databricksJobsPoolNodeType
    displayName: 'Azure Databricks Node Type in the Jobs Pool'
    type: string

  - name: databricksJobsPoolMinIdleInstances
    displayName: 'The minimum number of idle instances maintained by the Jobs Pool'
    type: number
    default: 0

  - name: databricksJobsPoolIdleInstanceAutotermination
    displayName: 'The number of minutes that idle instances are maintained by the Jobs Pool before being terminated'
    type: number
    default: 60

  - name: databricksSharedPoolName
    displayName: 'Name of the Azure Databricks Shared Instance Pool'
    type: string

  - name: databricksSharedPoolNodeType
    displayName: 'Azure Databricks Node Type in the Shared Pool'
    type: string

  - name: databricksSharedPoolMinIdleInstances
    displayName: 'The minimum number of idle instances maintained by the Shared Pool'
    type: number
    default: 0

  - name: databricksSharedPoolIdleInstanceAutotermination
    displayName: 'The number of minutes that idle instances are maintained by the Shared Pool before being terminated'
    type: number
    default: 120

  - name: databricksSharedClusterName
    displayName: 'Name of the Azure Databricks Shared Cluster'
    type: string

  - name: databricksSharedClusterNumWorkers
    displayName: 'Number of worker nodes in the Databricks Shared Cluster'
    type: number
    default: 1

  - name: databricksSharedClusterMaxNumWorkers
    displayName: 'Maximum number of worker nodes in the Databricks Shared Cluster'
    type: number
    default: 10

  - name: databricksSparkVersion
    displayName: 'Azure Databricks Spark Version'
    type: string
    default: '12.2.x-scala2.12'

  - name: databricksSingleNodeClusterPolicyLocation
    displayName: 'Location of the Cluster Policy json file'
    type: string

  - name: keyVaultName
    displayName: 'Azure Key Vault Name'
    type: string

  - name: databricksSecretScopeName
    displayName: 'Databricks Secret Scope Name'
    type: string

  - name: secretNameClientSecret
    displayName: 'Secret Name of the data pipeline Service Principal Client Secret'
    type: string
    default: 'spClientSecret'

  - name: notebooksSharedSourceLocation
    displayName: 'Location of generic notebooks to be deployed in a shared Databricks workspace folder'
    type: string

  - name: notebooksSharedWorkspaceFolder
    displayName: 'Folder path in the Databricks workspace where the shared notebooks will be deployed'
    type: string

  - name: mountAdlsNotebookPath
    displayName: 'Workspace path to the notebook that can mount ADLS Gen 2 Filesystems'
    type: string
    default: '/Shared/mount-adls-gen-2'

  - name: projectMountPoint
    displayName: 'DBFS mount location of the Project Filesystems'
    type: string

  - name: notebooksPipelineWorkspaceFolder
    displayName: 'Databricks workspace folder for the data pipeline notebooks'
    type: string

  - name: notebooksProjectWorkspaceFolder
    displayName: 'Databricks workspace folder for the Project notebooks'
    type: string

  - name: scriptsLocation
    displayName: 'Location of Scripts'
    type: string

steps:

  # Setup Python
  - template: 'configure-python.yml'  # Template reference

  # Get the Databricks workspace URL and AAD Access Token of the Azure DevOps Service Principal
  - template: 'get-workspace-login.yml'  # Template reference
    parameters:
      serviceConnection: ${{ parameters.serviceConnection }}
      resourceGroupName: ${{ parameters.resourceGroupName }}
      databricksWorkspaceName: ${{ parameters.databricksWorkspaceName }}
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Get the Principal ID and Object ID of the Data Factory Managed Identity
  - task: AzurePowerShell@5
    displayName: 'Get the Managed Identity of the Data Factory'
    inputs:
      azureSubscription: ${{ parameters.serviceConnection }}
      ScriptPath: '${{ parameters.scriptsLocation }}/get_data_factory_identity.ps1'
      ScriptArguments: '${{ parameters.resourceGroupName }} ${{ parameters.dataFactoryName }}'
      azurePowerShellVersion: 'LatestVersion'

  # Add the Azure Data Factory Service Principal to the Databricks workspace
  # The Service Principal must have 'allow_cluster_create' in order to create new job clusters as policies are not supported by ADF
  - task: Bash@3
    displayName: 'Add ADF Service Principal to the Databricks workspace'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/add_principal_to_workspace.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "service_principal" "$(adfPrincipalId)" "${{ parameters.dataFactoryName }}" "allow-cluster-create"'

  # Add the data pipeline Service Principal to the Databricks workspace
  - task: Bash@3
    displayName: 'Add data pipeline Service Principal to the Databricks workspace'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/add_principal_to_workspace.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "service_principal" "${{ parameters.dataServicePrincipalClientId }}" "${{ parameters.dataServicePrincipalClientId }}" "allow-cluster-create"'

  # Sync the AD Project group with the Databricks workspace
  - task: AzureCLI@2
    displayName: 'Sync group "${{ parameters.projectGroupName }}" to the Databricks workspace'
    inputs:
      azureSubscription: '${{ parameters.serviceConnection }}'
      scriptType: 'bash'
      scriptPath: '${{ parameters.scriptsLocation }}/sync_group.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.projectGroupName }}"'

  # Deploy Databricks generic notebooks in a Shared location
  - template: 'deploy-notebooks.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      notebooksSourceLocation: ${{ parameters.notebooksSharedSourceLocation }}
      notebooksWorkspaceFolder: ${{ parameters.notebooksSharedWorkspaceFolder }}

  # Create an empty local folder
  - task: Bash@3
    displayName: 'Create an empty local folder'
    inputs:
      targetType: 'inline'
      script: 'mkdir -p /tmp/empty'

  # Create an empty workspace folder for the Pipeline notebooks
  - template: 'deploy-notebooks.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      notebooksSourceLocation: "/tmp/empty"
      notebooksWorkspaceFolder: ${{ parameters.notebooksPipelineWorkspaceFolder }}

  # Get the workspace Object ID of the Pipeline notebooks folder
  - task: Bash@3
    displayName: 'Get workspace Object ID of ${{ parameters.notebooksPipelineWorkspaceFolder }}'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/get_workspace_object_id.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.notebooksPipelineWorkspaceFolder }}"'

  # Give CAN_MANAGE on the Pipeline notebooks folder to the data pipeline Service Principal
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'directories'
      databricksResourceId: $(workspaceObjectId)
      databricksPrincipalType: 'service_principal'
      databricksPrincipalId: ${{ parameters.dataServicePrincipalClientId }}
      databricksPermissionLevel: 'CAN_MANAGE'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Give CAN_RUN on the Pipeline notebooks folder to the Data Factory Managed Identity
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'directories'
      databricksResourceId: $(workspaceObjectId)
      databricksPrincipalType: 'service_principal'
      databricksPrincipalId: $(adfPrincipalId)
      databricksPermissionLevel: 'CAN_RUN'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Create an empty workspace folder for the Project notebooks
  - template: 'deploy-notebooks.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      notebooksSourceLocation: "/tmp/empty"
      notebooksWorkspaceFolder: ${{ parameters.notebooksProjectWorkspaceFolder }}

  # Get the Workspace Object ID of the Project notebooks folder
  - task: Bash@3
    displayName: 'Get Workspace Object ID of ${{ parameters.notebooksProjectWorkspaceFolder }}'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/get_workspace_object_id.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.notebooksProjectWorkspaceFolder }}"'

  # Give CAN_MANAGE on the Project notebooks folder to the Project group
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'directories'
      databricksResourceId: $(workspaceObjectId)
      databricksPrincipalType: 'group'
      databricksPrincipalId: ${{ parameters.projectGroupName }}
      databricksPermissionLevel: 'CAN_MANAGE'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Create the Databricks Secret Scope
  - task: Bash@3
    displayName: 'Create Databricks Secret Scope ${{ parameters.databricksSecretScopeName }}'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/create_secret_scope.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.databricksSecretScopeName }}"'

  # Give READ on the Secret Scope to the Azure Data Factory Managed Identity
  - task: Bash@3
    displayName: 'Set READ on ${{ parameters.databricksSecretScopeName }} to the Data Factory Managed Identity'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/create_secret_scope_acl.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.databricksSecretScopeName }}" "$(adfPrincipalId)" "READ"'

  # Give WRITE on the Secret Scope to the data pipeline Service Principal
  - task: Bash@3
    displayName: 'Set WRITE on ${{ parameters.databricksSecretScopeName }} to the data pipeline Service Principal'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/create_secret_scope_acl.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "${{ parameters.databricksSecretScopeName }}" "${{ parameters.dataServicePrincipalClientId }}" "WRITE"'

  # Deploy the Jobs Instance Pool
  - template: 'deploy-instance-pool.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksPoolName: ${{ parameters.databricksJobsPoolName }}
      databricksPoolNodeType: ${{ parameters.databricksJobsPoolNodeType }}
      databricksPoolNodeAvailability: 'ON_DEMAND_AZURE'
      databricksPoolMinIdleInstances: ${{ parameters.databricksJobsPoolMinIdleInstances }}
      databricksPoolIdleInstanceAutotermination: ${{ parameters.databricksJobsPoolIdleInstanceAutotermination }}
      databricksPoolSparkVersion: ${{ parameters.databricksSparkVersion }}
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Give CAN_ATTACH_TO on the Jobs Instance Pool to the Data Factory Managed Identity
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'instance-pools'
      databricksResourceId: $(databricksPoolId)
      databricksPrincipalType: 'service_principal'
      databricksPrincipalId: $(adfPrincipalId)
      databricksPermissionLevel: 'CAN_ATTACH_TO'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Give CAN_ATTACH_TO on the Jobs Instance Pool to the data pipeline Service Principal
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'instance-pools'
      databricksResourceId: $(databricksPoolId)
      databricksPrincipalType: 'service_principal'
      databricksPrincipalId: ${{ parameters.dataServicePrincipalClientId }}
      databricksPermissionLevel: 'CAN_ATTACH_TO'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Deploy the Shared Instance Pool
  - template: 'deploy-instance-pool.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksPoolName: ${{ parameters.databricksSharedPoolName }}
      databricksPoolNodeType: ${{ parameters.databricksSharedPoolNodeType }}
      databricksPoolNodeAvailability: 'SPOT_AZURE'
      databricksPoolMinIdleInstances: ${{ parameters.databricksSharedPoolMinIdleInstances }}
      databricksPoolIdleInstanceAutotermination: ${{ parameters.databricksSharedPoolIdleInstanceAutotermination }}
      databricksPoolSparkVersion: ${{ parameters.databricksSparkVersion }}
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Give CAN_ATTACH_TO on the Shared Instance Pool to the Project group
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'instance-pools'
      databricksResourceId: $(databricksPoolId)
      databricksPrincipalType: 'group'
      databricksPrincipalId: ${{ parameters.projectGroupName }}
      databricksPermissionLevel: 'CAN_ATTACH_TO'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Deploy the Shared Autoscaling Cluster
  - task: PythonScript@0
    displayName: 'Create Databricks Cluster ${{ parameters.databricksSharedClusterName }}'
    inputs:
      scriptSource: 'filePath'
      scriptPath: '${{ parameters.scriptsLocation }}/create_cluster.py'
      arguments: '"$(databricksWorkspaceUrl)"
                  "$(accessToken)"
                  "${{ parameters.databricksSharedClusterName }}"
                  "Credential Passthrough"
                  "120"
                  "${{ parameters.databricksSparkVersion }}"
                  "$(databricksPoolId)"
                  "${{ parameters.databricksSharedClusterNumWorkers }}"
                  "${{ parameters.databricksSharedClusterMaxNumWorkers }}"'

  # Give CAN_ATTACH_TO on the Shared Autoscaling Cluster to all users
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'clusters'
      databricksResourceId: $(databricksClusterId)
      databricksPrincipalType: 'group'
      databricksPrincipalId: 'users'
      databricksPermissionLevel: 'CAN_ATTACH_TO'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Give CAN_RESTART on the Shared Autoscaling Cluster to the Project group
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'clusters'
      databricksResourceId: $(databricksClusterId)
      databricksPrincipalType: 'group'
      databricksPrincipalId: ${{ parameters.projectGroupName }}
      databricksPermissionLevel: 'CAN_RESTART'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Deploy the Single Node Cluster Policy
  - task: Bash@3
    displayName: 'Create the Single Node Cluster Policy'
    inputs:
      targetType: 'filePath'
      filePath: '${{ parameters.scriptsLocation }}/create_cluster_policy.sh'
      arguments: '"$(databricksWorkspaceUrl)" "$(accessToken)" "Single Node Cluster" "${{ parameters.databricksSingleNodeClusterPolicyLocation }}"'

  # Give CAN_USE on the Single Node Cluster Policy to the Project group
  - template: 'set-databricks-permission.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksResourceType: 'cluster-policies'
      databricksResourceId: $(databricksPolicyId)
      databricksPrincipalType: 'group'
      databricksPrincipalId: ${{ parameters.projectGroupName }}
      databricksPermissionLevel: 'CAN_USE'
      scriptsLocation: ${{ parameters.scriptsLocation }}

  # Get the data pipeline Service Principal Client Secret from the Key Vault
  - task: AzureCLI@2
    inputs:
      azureSubscription: ${{ parameters.serviceConnection }}
      scriptType: 'bash'
      scriptLocation: 'inlineScript'
      inlineScript: |
        spClientSecret=$(az keyvault secret show --name "${{ parameters.secretNameClientSecret }}" --vault-name "${{ parameters.keyVaultName }}" --query value)
        [ -n "${spClientSecret}" ] && echo "##vso[task.setvariable variable=spClientSecret;issecret=true]${spClientSecret}" || exit 1

  # Add the secret from Key Vault to the Databricks Secret Scope
  # This needs to be done until Key Vault backed Secret Scopes are supported with Service Principals
  - task: AzureCLI@2
    displayName: 'Add the Client Secret to Databricks Secret Scope'
    inputs:
      azureSubscription: ${{ parameters.serviceConnection }}
      scriptType: 'bash'
      scriptPath: '${{ parameters.scriptsLocation }}/add_secret_to_secret_scope.sh'
      arguments: '"$(databricksWorkspaceUrl)"
                  "$(accessToken)"
                  "${{ parameters.databricksSecretScopeName }}"
                  "${{ parameters.secretNameClientSecret }}"
                  "$(spClientSecret)"'

  # Get the Azure AD Tenant ID
  - task: AzureCLI@2
    displayName: 'Get the Tenant ID'
    inputs:
      azureSubscription: ${{ parameters.serviceConnection }}
      addSpnToEnvironment: true
      scriptType: 'bash'
      scriptLocation: 'inlineScript'
      inlineScript: |
        [ -n "${tenantId}" ] && echo "##vso[task.setvariable variable=tenantId;issecret=false]${tenantId}" || exit 1

  # Mount the ADLS Gen2 Project Filesystem using the latest Client Secret of the data pipeline Service Principal
  - template: 'run-notebook-job.yml'  # Template reference
    parameters:
      databricksWorkspaceUrl: $(databricksWorkspaceUrl)
      accessToken: $(accessToken)
      databricksClusterNodeTypeOrPool: $(databricksPoolId)
      notebookPath: ${{ parameters.mountAdlsNotebookPath }}
      notebookParameters: '{\"secretScopeName\":\"${{ parameters.databricksSecretScopeName }}\",
                            \"spClientId\":\"${{ parameters.dataServicePrincipalClientId }}\",
                            \"secretNameClientSecret\":\"${{ parameters.secretNameClientSecret }}\",
                            \"tenantId\":\"$(tenantId)\",
                            \"storageAccountName\":\"${{ parameters.storageAccountName }}\",
                            \"storageContainerName\":\"${{ parameters.projectContainerName }}\",
                            \"mountPoint\":\"${{ parameters.projectMountPoint }}\"}'
      scriptsLocation: ${{ parameters.scriptsLocation }}
